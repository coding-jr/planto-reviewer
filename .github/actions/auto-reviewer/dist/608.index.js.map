{"version":3,"file":"608.index.js","mappings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vIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1HA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://typescript-action/./node_modules/langchain/dist/chains/combine_docs_chain.js","webpack://typescript-action/./node_modules/langchain/dist/chains/llm_chain.js","webpack://typescript-action/./node_modules/langchain/dist/prompts/base.js","webpack://typescript-action/./node_modules/langchain/dist/prompts/prompt.js","webpack://typescript-action/./node_modules/langchain/dist/prompts/template.js"],"sourcesContent":["import { BaseChain } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\n/**\n * Chain that combines documents by stuffing into context.\n * @augments BaseChain\n * @augments StuffDocumentsChainInput\n */\nexport class StuffDocumentsChain extends BaseChain {\n    get inputKeys() {\n        return [this.inputKey, ...this.llmChain.inputKeys];\n    }\n    get outputKeys() {\n        return this.llmChain.outputKeys;\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"llmChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"inputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"input_documents\"\n        });\n        Object.defineProperty(this, \"documentVariableName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"context\"\n        });\n        this.llmChain = fields.llmChain;\n        this.documentVariableName =\n            fields.documentVariableName ?? this.documentVariableName;\n        this.inputKey = fields.inputKey ?? this.inputKey;\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        if (!(this.inputKey in values)) {\n            throw new Error(`Document key ${this.inputKey} not found.`);\n        }\n        const { [this.inputKey]: docs, ...rest } = values;\n        const texts = docs.map(({ pageContent }) => pageContent);\n        const text = texts.join(\"\\n\\n\");\n        const result = await this.llmChain.call({\n            ...rest,\n            [this.documentVariableName]: text,\n        }, runManager?.getChild());\n        return result;\n    }\n    _chainType() {\n        return \"stuff_documents_chain\";\n    }\n    static async deserialize(data) {\n        if (!data.llm_chain) {\n            throw new Error(\"Missing llm_chain\");\n        }\n        return new StuffDocumentsChain({\n            llmChain: await LLMChain.deserialize(data.llm_chain),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm_chain: this.llmChain.serialize(),\n        };\n    }\n}\n/**\n * Combine documents by mapping a chain over them, then combining results.\n * @augments BaseChain\n * @augments StuffDocumentsChainInput\n */\nexport class MapReduceDocumentsChain extends BaseChain {\n    get inputKeys() {\n        return [this.inputKey, ...this.combineDocumentChain.inputKeys];\n    }\n    get outputKeys() {\n        return this.combineDocumentChain.outputKeys;\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"llmChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"inputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"input_documents\"\n        });\n        Object.defineProperty(this, \"documentVariableName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"context\"\n        });\n        Object.defineProperty(this, \"returnIntermediateSteps\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 3000\n        });\n        Object.defineProperty(this, \"maxIterations\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 10\n        });\n        Object.defineProperty(this, \"ensureMapStep\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"combineDocumentChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.llmChain = fields.llmChain;\n        this.combineDocumentChain = fields.combineDocumentChain;\n        this.documentVariableName =\n            fields.documentVariableName ?? this.documentVariableName;\n        this.ensureMapStep = fields.ensureMapStep ?? this.ensureMapStep;\n        this.inputKey = fields.inputKey ?? this.inputKey;\n        this.maxTokens = fields.maxTokens ?? this.maxTokens;\n        this.maxIterations = fields.maxIterations ?? this.maxIterations;\n        this.returnIntermediateSteps = fields.returnIntermediateSteps ?? false;\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        if (!(this.inputKey in values)) {\n            throw new Error(`Document key ${this.inputKey} not found.`);\n        }\n        const { [this.inputKey]: docs, ...rest } = values;\n        let currentDocs = docs;\n        let intermediateSteps = [];\n        // For each iteration, we'll use the `llmChain` to get a new result\n        for (let i = 0; i < this.maxIterations; i += 1) {\n            const inputs = currentDocs.map((d) => ({\n                [this.documentVariableName]: d.pageContent,\n                ...rest,\n            }));\n            // Calculate the total tokens required in the input\n            const promises = inputs.map(async (i) => {\n                const prompt = await this.llmChain.prompt.format(i);\n                return this.llmChain.llm.getNumTokens(prompt);\n            });\n            const length = await Promise.all(promises).then((results) => results.reduce((a, b) => a + b, 0));\n            const canSkipMapStep = i !== 0 || !this.ensureMapStep;\n            const withinTokenLimit = length < this.maxTokens;\n            // If we can skip the map step, and we're within the token limit, we don't\n            // need to run the map step, so just break out of the loop.\n            if (canSkipMapStep && withinTokenLimit) {\n                break;\n            }\n            const results = await this.llmChain.apply(inputs, runManager ? [runManager.getChild()] : undefined);\n            const { outputKey } = this.llmChain;\n            // If the flag is set, then concat that to the intermediate steps\n            if (this.returnIntermediateSteps) {\n                intermediateSteps = intermediateSteps.concat(results.map((r) => r[outputKey]));\n            }\n            currentDocs = results.map((r) => ({\n                pageContent: r[outputKey],\n            }));\n        }\n        // Now, with the final result of all the inputs from the `llmChain`, we can\n        // run the `combineDocumentChain` over them.\n        const newInputs = { input_documents: currentDocs, ...rest };\n        const result = await this.combineDocumentChain.call(newInputs, runManager?.getChild());\n        // Return the intermediate steps results if the flag is set\n        if (this.returnIntermediateSteps) {\n            return { ...result, intermediateSteps };\n        }\n        return result;\n    }\n    _chainType() {\n        return \"map_reduce_documents_chain\";\n    }\n    static async deserialize(data) {\n        if (!data.llm_chain) {\n            throw new Error(\"Missing llm_chain\");\n        }\n        if (!data.combine_document_chain) {\n            throw new Error(\"Missing combine_document_chain\");\n        }\n        return new MapReduceDocumentsChain({\n            llmChain: await LLMChain.deserialize(data.llm_chain),\n            combineDocumentChain: await BaseChain.deserialize(data.combine_document_chain),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm_chain: this.llmChain.serialize(),\n            combine_document_chain: this.combineDocumentChain.serialize(),\n        };\n    }\n}\n/**\n * Combine documents by doing a first pass and then refining on more documents.\n * @augments BaseChain\n * @augments RefineDocumentsChainInput\n */\nexport class RefineDocumentsChain extends BaseChain {\n    get defaultDocumentPrompt() {\n        return new PromptTemplate({\n            inputVariables: [\"page_content\"],\n            template: \"{page_content}\",\n        });\n    }\n    get inputKeys() {\n        return [this.inputKey, ...this.refineLLMChain.inputKeys];\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"llmChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"inputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"input_documents\"\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"output_text\"\n        });\n        Object.defineProperty(this, \"documentVariableName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"context\"\n        });\n        Object.defineProperty(this, \"initialResponseName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"existing_answer\"\n        });\n        Object.defineProperty(this, \"refineLLMChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"documentPrompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: this.defaultDocumentPrompt\n        });\n        this.llmChain = fields.llmChain;\n        this.refineLLMChain = fields.refineLLMChain;\n        this.documentVariableName =\n            fields.documentVariableName ?? this.documentVariableName;\n        this.inputKey = fields.inputKey ?? this.inputKey;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.documentPrompt = fields.documentPrompt ?? this.documentPrompt;\n        this.initialResponseName =\n            fields.initialResponseName ?? this.initialResponseName;\n    }\n    /** @ignore */\n    async _constructInitialInputs(doc, rest) {\n        const baseInfo = {\n            page_content: doc.pageContent,\n            ...doc.metadata,\n        };\n        const documentInfo = {};\n        this.documentPrompt.inputVariables.forEach((value) => {\n            documentInfo[value] = baseInfo[value];\n        });\n        const baseInputs = {\n            [this.documentVariableName]: await this.documentPrompt.format({\n                ...documentInfo,\n            }),\n        };\n        const inputs = { ...baseInputs, ...rest };\n        return inputs;\n    }\n    /** @ignore */\n    async _constructRefineInputs(doc, res) {\n        const baseInfo = {\n            page_content: doc.pageContent,\n            ...doc.metadata,\n        };\n        const documentInfo = {};\n        this.documentPrompt.inputVariables.forEach((value) => {\n            documentInfo[value] = baseInfo[value];\n        });\n        const baseInputs = {\n            [this.documentVariableName]: await this.documentPrompt.format({\n                ...documentInfo,\n            }),\n        };\n        const inputs = { [this.initialResponseName]: res, ...baseInputs };\n        return inputs;\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        if (!(this.inputKey in values)) {\n            throw new Error(`Document key ${this.inputKey} not found.`);\n        }\n        const { [this.inputKey]: docs, ...rest } = values;\n        const currentDocs = docs;\n        const initialInputs = await this._constructInitialInputs(currentDocs[0], rest);\n        let res = await this.llmChain.predict({ ...initialInputs }, runManager?.getChild());\n        const refineSteps = [res];\n        for (let i = 1; i < currentDocs.length; i += 1) {\n            const refineInputs = await this._constructRefineInputs(currentDocs[i], res);\n            const inputs = { ...refineInputs, ...rest };\n            res = await this.refineLLMChain.predict({ ...inputs }, runManager?.getChild());\n            refineSteps.push(res);\n        }\n        return { [this.outputKey]: res };\n    }\n    _chainType() {\n        return \"refine_documents_chain\";\n    }\n    static async deserialize(data) {\n        const SerializedLLMChain = data.llm_chain;\n        if (!SerializedLLMChain) {\n            throw new Error(\"Missing llm_chain\");\n        }\n        const SerializedRefineDocumentChain = data.refine_llm_chain;\n        if (!SerializedRefineDocumentChain) {\n            throw new Error(\"Missing refine_llm_chain\");\n        }\n        return new RefineDocumentsChain({\n            llmChain: await LLMChain.deserialize(SerializedLLMChain),\n            refineLLMChain: await LLMChain.deserialize(SerializedRefineDocumentChain),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm_chain: this.llmChain.serialize(),\n            refine_llm_chain: this.refineLLMChain.serialize(),\n        };\n    }\n}\n","import { BaseChain } from \"./base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n *\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser = fields.outputParser ?? this.outputParser;\n        if (this.prompt.outputParser) {\n            if (this.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    /** @ignore */\n    async _getFinalOutput(generations, promptValue, runManager) {\n        const completion = generations[0].text;\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseWithPrompt(completion, promptValue, runManager?.getChild());\n        }\n        else {\n            finalCompletion = completion;\n        }\n        return finalCompletion;\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    call(values, callbacks) {\n        return super.call(values, callbacks);\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        const valuesForPrompt = { ...values };\n        const valuesForLLM = {};\n        for (const key of this.llm.callKeys) {\n            if (key in values) {\n                valuesForLLM[key] = values[key];\n                delete valuesForPrompt[key];\n            }\n        }\n        const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n        const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n        return {\n            [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager),\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @param callbackManager - CallbackManager to use\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values, callbackManager) {\n        const output = await this.call(values, callbackManager);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm_chain\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm: this.llm.serialize(),\n            prompt: this.prompt.serialize(),\n        };\n    }\n}\n","import { BasePromptValue, HumanChatMessage, } from \"../schema/index.js\";\nexport class StringPromptValue extends BasePromptValue {\n    constructor(value) {\n        super();\n        Object.defineProperty(this, \"value\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.value = value;\n    }\n    toString() {\n        return this.value;\n    }\n    toChatMessages() {\n        return [new HumanChatMessage(this.value)];\n    }\n}\n/**\n * Base class for prompt templates. Exposes a format method that returns a\n * string prompt given a set of input values.\n */\nexport class BasePromptTemplate {\n    constructor(input) {\n        Object.defineProperty(this, \"inputVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"partialVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const { inputVariables } = input;\n        if (inputVariables.includes(\"stop\")) {\n            throw new Error(\"Cannot have an input variable named 'stop', as it is used internally, please rename.\");\n        }\n        Object.assign(this, input);\n    }\n    async mergePartialAndUserVariables(userVariables) {\n        const partialVariables = this.partialVariables ?? {};\n        const partialValues = {};\n        for (const [key, value] of Object.entries(partialVariables)) {\n            if (typeof value === \"string\") {\n                partialValues[key] = value;\n            }\n            else {\n                partialValues[key] = await value();\n            }\n        }\n        const allKwargs = { ...partialValues, ...userVariables };\n        return allKwargs;\n    }\n    /**\n     * Load a prompt template from a json-like object describing it.\n     *\n     * @remarks\n     * Deserializing needs to be async because templates (e.g. {@link FewShotPromptTemplate}) can\n     * reference remote resources that we read asynchronously with a web\n     * request.\n     */\n    static async deserialize(data) {\n        switch (data._type) {\n            case \"prompt\": {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize(data);\n            }\n            case undefined: {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize({ ...data, _type: \"prompt\" });\n            }\n            case \"few_shot\": {\n                const { FewShotPromptTemplate } = await import(\"./few_shot.js\");\n                return FewShotPromptTemplate.deserialize(data);\n            }\n            default:\n                throw new Error(`Invalid prompt type in config: ${data._type}`);\n        }\n    }\n}\nexport class BaseStringPromptTemplate extends BasePromptTemplate {\n    async formatPromptValue(values) {\n        const formattedPrompt = await this.format(values);\n        return new StringPromptValue(formattedPrompt);\n    }\n}\n/**\n * Base class for example selectors.\n */\nexport class BaseExampleSelector {\n}\n","import { BaseStringPromptTemplate } from \"./base.js\";\nimport { checkValidTemplate, parseTemplate, renderTemplate, } from \"./template.js\";\n/**\n * Schema to represent a basic prompt for an LLM.\n * @augments BasePromptTemplate\n * @augments PromptTemplateInput\n *\n * @example\n * ```ts\n * import { PromptTemplate } from \"langchain/prompts\";\n *\n * const prompt = new PromptTemplate({\n *   inputVariables: [\"foo\"],\n *   template: \"Say {foo}\",\n * });\n * ```\n */\nexport class PromptTemplate extends BaseStringPromptTemplate {\n    constructor(input) {\n        super(input);\n        Object.defineProperty(this, \"template\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"templateFormat\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"f-string\"\n        });\n        Object.defineProperty(this, \"validateTemplate\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.assign(this, input);\n        if (this.validateTemplate) {\n            let totalInputVariables = this.inputVariables;\n            if (this.partialVariables) {\n                totalInputVariables = totalInputVariables.concat(Object.keys(this.partialVariables));\n            }\n            checkValidTemplate(this.template, this.templateFormat, totalInputVariables);\n        }\n    }\n    _getPromptType() {\n        return \"prompt\";\n    }\n    async format(values) {\n        const allValues = await this.mergePartialAndUserVariables(values);\n        return renderTemplate(this.template, this.templateFormat, allValues);\n    }\n    /**\n     * Take examples in list format with prefix and suffix to create a prompt.\n     *\n     * Intendend to be used a a way to dynamically create a prompt from examples.\n     *\n     * @param examples - List of examples to use in the prompt.\n     * @param suffix - String to go after the list of examples. Should generally set up the user's input.\n     * @param inputVariables - A list of variable names the final prompt template will expect\n     * @param exampleSeparator - The separator to use in between examples\n     * @param prefix - String that should go before any examples. Generally includes examples.\n     *\n     * @returns The final prompt template generated.\n     */\n    static fromExamples(examples, suffix, inputVariables, exampleSeparator = \"\\n\\n\", prefix = \"\") {\n        const template = [prefix, ...examples, suffix].join(exampleSeparator);\n        return new PromptTemplate({\n            inputVariables,\n            template,\n        });\n    }\n    /**\n     * Load prompt template from a template f-string\n     */\n    static fromTemplate(template, { templateFormat = \"f-string\", ...rest } = {}) {\n        const names = new Set();\n        parseTemplate(template, templateFormat).forEach((node) => {\n            if (node.type === \"variable\") {\n                names.add(node.name);\n            }\n        });\n        return new PromptTemplate({\n            inputVariables: [...names],\n            templateFormat,\n            template,\n            ...rest,\n        });\n    }\n    async partial(values) {\n        const promptDict = { ...this };\n        promptDict.inputVariables = this.inputVariables.filter((iv) => !(iv in values));\n        promptDict.partialVariables = {\n            ...(this.partialVariables ?? {}),\n            ...values,\n        };\n        return new PromptTemplate(promptDict);\n    }\n    serialize() {\n        if (this.outputParser !== undefined) {\n            throw new Error(\"Cannot serialize a prompt template with an output parser\");\n        }\n        return {\n            _type: this._getPromptType(),\n            input_variables: this.inputVariables,\n            template: this.template,\n            template_format: this.templateFormat,\n        };\n    }\n    static async deserialize(data) {\n        if (!data.template) {\n            throw new Error(\"Prompt template must have a template\");\n        }\n        const res = new PromptTemplate({\n            inputVariables: data.input_variables,\n            template: data.template,\n            templateFormat: data.template_format,\n        });\n        return res;\n    }\n}\n","export const parseFString = (template) => {\n    // Core logic replicated from internals of pythons built in Formatter class.\n    // https://github.com/python/cpython/blob/135ec7cefbaffd516b77362ad2b2ad1025af462e/Objects/stringlib/unicode_format.h#L700-L706\n    const chars = template.split(\"\");\n    const nodes = [];\n    const nextBracket = (bracket, start) => {\n        for (let i = start; i < chars.length; i += 1) {\n            if (bracket.includes(chars[i])) {\n                return i;\n            }\n        }\n        return -1;\n    };\n    let i = 0;\n    while (i < chars.length) {\n        if (chars[i] === \"{\" && i + 1 < chars.length && chars[i + 1] === \"{\") {\n            nodes.push({ type: \"literal\", text: \"{\" });\n            i += 2;\n        }\n        else if (chars[i] === \"}\" &&\n            i + 1 < chars.length &&\n            chars[i + 1] === \"}\") {\n            nodes.push({ type: \"literal\", text: \"}\" });\n            i += 2;\n        }\n        else if (chars[i] === \"{\") {\n            const j = nextBracket(\"}\", i);\n            if (j < 0) {\n                throw new Error(\"Unclosed '{' in template.\");\n            }\n            nodes.push({\n                type: \"variable\",\n                name: chars.slice(i + 1, j).join(\"\"),\n            });\n            i = j + 1;\n        }\n        else if (chars[i] === \"}\") {\n            throw new Error(\"Single '}' in template.\");\n        }\n        else {\n            const next = nextBracket(\"{}\", i);\n            const text = (next < 0 ? chars.slice(i) : chars.slice(i, next)).join(\"\");\n            nodes.push({ type: \"literal\", text });\n            i = next < 0 ? chars.length : next;\n        }\n    }\n    return nodes;\n};\nexport const interpolateFString = (template, values) => parseFString(template).reduce((res, node) => {\n    if (node.type === \"variable\") {\n        if (node.name in values) {\n            return res + values[node.name];\n        }\n        throw new Error(`Missing value for input ${node.name}`);\n    }\n    return res + node.text;\n}, \"\");\nexport const DEFAULT_FORMATTER_MAPPING = {\n    \"f-string\": interpolateFString,\n    jinja2: (_, __) => \"\",\n};\nexport const DEFAULT_PARSER_MAPPING = {\n    \"f-string\": parseFString,\n    jinja2: (_) => [],\n};\nexport const renderTemplate = (template, templateFormat, inputValues) => DEFAULT_FORMATTER_MAPPING[templateFormat](template, inputValues);\nexport const parseTemplate = (template, templateFormat) => DEFAULT_PARSER_MAPPING[templateFormat](template);\nexport const checkValidTemplate = (template, templateFormat, inputVariables) => {\n    if (!(templateFormat in DEFAULT_FORMATTER_MAPPING)) {\n        const validFormats = Object.keys(DEFAULT_FORMATTER_MAPPING);\n        throw new Error(`Invalid template format. Got \\`${templateFormat}\\`;\n                         should be one of ${validFormats}`);\n    }\n    try {\n        const dummyInputs = inputVariables.reduce((acc, v) => {\n            acc[v] = \"foo\";\n            return acc;\n        }, {});\n        renderTemplate(template, templateFormat, dummyInputs);\n    }\n    catch {\n        throw new Error(\"Invalid prompt schema.\");\n    }\n};\n"],"names":[],"sourceRoot":""}
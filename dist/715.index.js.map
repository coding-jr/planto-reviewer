{"version":3,"file":"715.index.js","mappings":";;;;;;;;;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC5HA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://typescript-action/./node_modules/langchain/dist/chains/base.js","webpack://typescript-action/./node_modules/langchain/dist/chains/llm_chain.js","webpack://typescript-action/./node_modules/langchain/dist/prompts/base.js"],"sourcesContent":["import { RUN_KEY } from \"../schema/index.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nimport { BaseLangChain } from \"../base_language/index.js\";\n/**\n * Base interface that all chains must implement.\n */\nexport class BaseChain extends BaseLangChain {\n    constructor(fields, \n    /** @deprecated */\n    verbose, \n    /** @deprecated */\n    callbacks) {\n        if (arguments.length === 1 &&\n            typeof fields === \"object\" &&\n            !(\"saveContext\" in fields)) {\n            // fields is not a BaseMemory\n            const { memory, callbackManager, ...rest } = fields;\n            super({ ...rest, callbacks: callbackManager ?? rest.callbacks });\n            this.memory = memory;\n        }\n        else {\n            // fields is a BaseMemory\n            super({ verbose, callbacks });\n            this.memory = fields;\n        }\n    }\n    /**\n     * Return a json-like object representing this chain.\n     */\n    serialize() {\n        throw new Error(\"Method not implemented.\");\n    }\n    async run(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    input, callbacks) {\n        const isKeylessInput = this.inputKeys.length <= 1;\n        if (!isKeylessInput) {\n            throw new Error(`Chain ${this._chainType()} expects multiple inputs, cannot use 'run' `);\n        }\n        const values = this.inputKeys.length ? { [this.inputKeys[0]]: input } : {};\n        const returnValues = await this.call(values, callbacks);\n        const keys = Object.keys(returnValues);\n        if (keys.length === 1) {\n            return returnValues[keys[0]];\n        }\n        throw new Error(\"return values have multiple keys, `run` only supported when one key currently\");\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    async call(values, callbacks) {\n        const fullValues = { ...values };\n        if (!(this.memory == null)) {\n            const newValues = await this.memory.loadMemoryVariables(values);\n            for (const [key, value] of Object.entries(newValues)) {\n                fullValues[key] = value;\n            }\n        }\n        const callbackManager_ = await CallbackManager.configure(callbacks, this.callbacks, { verbose: this.verbose });\n        const runManager = await callbackManager_?.handleChainStart({ name: this._chainType() }, fullValues);\n        let outputValues;\n        try {\n            outputValues = await this._call(fullValues, runManager);\n        }\n        catch (e) {\n            await runManager?.handleChainError(e);\n            throw e;\n        }\n        if (!(this.memory == null)) {\n            await this.memory.saveContext(values, outputValues);\n        }\n        await runManager?.handleChainEnd(outputValues);\n        // add the runManager's currentRunId to the outputValues\n        Object.defineProperty(outputValues, RUN_KEY, {\n            value: runManager ? { runId: runManager?.runId } : undefined,\n            configurable: true,\n        });\n        return outputValues;\n    }\n    /**\n     * Call the chain on all inputs in the list\n     */\n    async apply(inputs, callbacks) {\n        return Promise.all(inputs.map(async (i, idx) => this.call(i, callbacks?.[idx])));\n    }\n    /**\n     * Load a chain from a json-like object describing it.\n     */\n    static async deserialize(data, values = {}) {\n        switch (data._type) {\n            case \"llm_chain\": {\n                const { LLMChain } = await import(\"./llm_chain.js\");\n                return LLMChain.deserialize(data);\n            }\n            case \"sequential_chain\": {\n                const { SequentialChain } = await import(\"./sequential_chain.js\");\n                return SequentialChain.deserialize(data);\n            }\n            case \"simple_sequential_chain\": {\n                const { SimpleSequentialChain } = await import(\"./sequential_chain.js\");\n                return SimpleSequentialChain.deserialize(data);\n            }\n            case \"stuff_documents_chain\": {\n                const { StuffDocumentsChain } = await import(\"./combine_docs_chain.js\");\n                return StuffDocumentsChain.deserialize(data);\n            }\n            case \"map_reduce_documents_chain\": {\n                const { MapReduceDocumentsChain } = await import(\"./combine_docs_chain.js\");\n                return MapReduceDocumentsChain.deserialize(data);\n            }\n            case \"refine_documents_chain\": {\n                const { RefineDocumentsChain } = await import(\"./combine_docs_chain.js\");\n                return RefineDocumentsChain.deserialize(data);\n            }\n            case \"vector_db_qa\": {\n                const { VectorDBQAChain } = await import(\"./vector_db_qa.js\");\n                return VectorDBQAChain.deserialize(data, values);\n            }\n            default:\n                throw new Error(`Invalid prompt type in config: ${data._type}`);\n        }\n    }\n}\n","import { BaseChain } from \"./base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n *\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser = fields.outputParser ?? this.outputParser;\n        if (this.prompt.outputParser) {\n            if (this.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    /** @ignore */\n    async _getFinalOutput(generations, promptValue, runManager) {\n        const completion = generations[0].text;\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseWithPrompt(completion, promptValue, runManager?.getChild());\n        }\n        else {\n            finalCompletion = completion;\n        }\n        return finalCompletion;\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    call(values, callbacks) {\n        return super.call(values, callbacks);\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        const valuesForPrompt = { ...values };\n        const valuesForLLM = {};\n        for (const key of this.llm.callKeys) {\n            if (key in values) {\n                valuesForLLM[key] = values[key];\n                delete valuesForPrompt[key];\n            }\n        }\n        const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n        const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n        return {\n            [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager),\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @param callbackManager - CallbackManager to use\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values, callbackManager) {\n        const output = await this.call(values, callbackManager);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm_chain\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm: this.llm.serialize(),\n            prompt: this.prompt.serialize(),\n        };\n    }\n}\n","import { BasePromptValue, HumanChatMessage, } from \"../schema/index.js\";\nexport class StringPromptValue extends BasePromptValue {\n    constructor(value) {\n        super();\n        Object.defineProperty(this, \"value\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.value = value;\n    }\n    toString() {\n        return this.value;\n    }\n    toChatMessages() {\n        return [new HumanChatMessage(this.value)];\n    }\n}\n/**\n * Base class for prompt templates. Exposes a format method that returns a\n * string prompt given a set of input values.\n */\nexport class BasePromptTemplate {\n    constructor(input) {\n        Object.defineProperty(this, \"inputVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"partialVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const { inputVariables } = input;\n        if (inputVariables.includes(\"stop\")) {\n            throw new Error(\"Cannot have an input variable named 'stop', as it is used internally, please rename.\");\n        }\n        Object.assign(this, input);\n    }\n    async mergePartialAndUserVariables(userVariables) {\n        const partialVariables = this.partialVariables ?? {};\n        const partialValues = {};\n        for (const [key, value] of Object.entries(partialVariables)) {\n            if (typeof value === \"string\") {\n                partialValues[key] = value;\n            }\n            else {\n                partialValues[key] = await value();\n            }\n        }\n        const allKwargs = { ...partialValues, ...userVariables };\n        return allKwargs;\n    }\n    /**\n     * Load a prompt template from a json-like object describing it.\n     *\n     * @remarks\n     * Deserializing needs to be async because templates (e.g. {@link FewShotPromptTemplate}) can\n     * reference remote resources that we read asynchronously with a web\n     * request.\n     */\n    static async deserialize(data) {\n        switch (data._type) {\n            case \"prompt\": {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize(data);\n            }\n            case undefined: {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize({ ...data, _type: \"prompt\" });\n            }\n            case \"few_shot\": {\n                const { FewShotPromptTemplate } = await import(\"./few_shot.js\");\n                return FewShotPromptTemplate.deserialize(data);\n            }\n            default:\n                throw new Error(`Invalid prompt type in config: ${data._type}`);\n        }\n    }\n}\nexport class BaseStringPromptTemplate extends BasePromptTemplate {\n    async formatPromptValue(values) {\n        const formattedPrompt = await this.format(values);\n        return new StringPromptValue(formattedPrompt);\n    }\n}\n/**\n * Base class for example selectors.\n */\nexport class BaseExampleSelector {\n}\n"],"names":[],"sourceRoot":""}
{"version":3,"file":"726.index.js","mappings":";;;;;;;;;;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://typescript-action/./node_modules/langchain/dist/chains/llm_chain.js","webpack://typescript-action/./node_modules/langchain/dist/prompts/base.js"],"sourcesContent":["import { BaseChain } from \"./base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\n/**\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n *\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser = fields.outputParser ?? this.outputParser;\n        if (this.prompt.outputParser) {\n            if (this.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    /** @ignore */\n    async _getFinalOutput(generations, promptValue, runManager) {\n        const completion = generations[0].text;\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseWithPrompt(completion, promptValue, runManager?.getChild());\n        }\n        else {\n            finalCompletion = completion;\n        }\n        return finalCompletion;\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    call(values, callbacks) {\n        return super.call(values, callbacks);\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        const valuesForPrompt = { ...values };\n        const valuesForLLM = {};\n        for (const key of this.llm.callKeys) {\n            if (key in values) {\n                valuesForLLM[key] = values[key];\n                delete valuesForPrompt[key];\n            }\n        }\n        const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n        const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n        return {\n            [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager),\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @param callbackManager - CallbackManager to use\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values, callbackManager) {\n        const output = await this.call(values, callbackManager);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm_chain\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            llm: this.llm.serialize(),\n            prompt: this.prompt.serialize(),\n        };\n    }\n}\n","import { BasePromptValue, HumanChatMessage, } from \"../schema/index.js\";\nexport class StringPromptValue extends BasePromptValue {\n    constructor(value) {\n        super();\n        Object.defineProperty(this, \"value\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.value = value;\n    }\n    toString() {\n        return this.value;\n    }\n    toChatMessages() {\n        return [new HumanChatMessage(this.value)];\n    }\n}\n/**\n * Base class for prompt templates. Exposes a format method that returns a\n * string prompt given a set of input values.\n */\nexport class BasePromptTemplate {\n    constructor(input) {\n        Object.defineProperty(this, \"inputVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"partialVariables\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        const { inputVariables } = input;\n        if (inputVariables.includes(\"stop\")) {\n            throw new Error(\"Cannot have an input variable named 'stop', as it is used internally, please rename.\");\n        }\n        Object.assign(this, input);\n    }\n    async mergePartialAndUserVariables(userVariables) {\n        const partialVariables = this.partialVariables ?? {};\n        const partialValues = {};\n        for (const [key, value] of Object.entries(partialVariables)) {\n            if (typeof value === \"string\") {\n                partialValues[key] = value;\n            }\n            else {\n                partialValues[key] = await value();\n            }\n        }\n        const allKwargs = { ...partialValues, ...userVariables };\n        return allKwargs;\n    }\n    /**\n     * Load a prompt template from a json-like object describing it.\n     *\n     * @remarks\n     * Deserializing needs to be async because templates (e.g. {@link FewShotPromptTemplate}) can\n     * reference remote resources that we read asynchronously with a web\n     * request.\n     */\n    static async deserialize(data) {\n        switch (data._type) {\n            case \"prompt\": {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize(data);\n            }\n            case undefined: {\n                const { PromptTemplate } = await import(\"./prompt.js\");\n                return PromptTemplate.deserialize({ ...data, _type: \"prompt\" });\n            }\n            case \"few_shot\": {\n                const { FewShotPromptTemplate } = await import(\"./few_shot.js\");\n                return FewShotPromptTemplate.deserialize(data);\n            }\n            default:\n                throw new Error(`Invalid prompt type in config: ${data._type}`);\n        }\n    }\n}\nexport class BaseStringPromptTemplate extends BasePromptTemplate {\n    async formatPromptValue(values) {\n        const formattedPrompt = await this.format(values);\n        return new StringPromptValue(formattedPrompt);\n    }\n}\n/**\n * Base class for example selectors.\n */\nexport class BaseExampleSelector {\n}\n"],"names":[],"sourceRoot":""}